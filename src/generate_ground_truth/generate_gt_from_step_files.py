# idea of this file is:
# a) step files of screws are placed in a true folder
# b) step files of non_screw_parts are placed in a false folder
# dataset is generated by transformation via point_cloud to numpy array
import os.path
import numpy as np
import multiprocessing
from functools import partial
from src.point_cloud.stl_to_multiview import stl_to_multiview
from build_dictionaries import make_screw_gt_dictionary


def build_dataset(path, num_views=3, view_size=80, chunk_size=100):  # saving every chunk_size samples reduces memory usage
    data = []
    labels = []
    dict_path = make_screw_gt_dictionary(path, 'screw', 'no_screw')

    # mkdir
    chunk_dir = os.path.join(path, "chunks")
    os.makedirs(chunk_dir, exist_ok=True)

    pool = multiprocessing.Pool()

    with open(dict_path, mode='r') as file:
        sample_count = 0
        chunk_index = 0
        lines = file.readlines()

        results = pool.map(partial(process_sample, view_size=view_size), lines)

        for i, result in enumerate(results):
            data_batch, labels_batch = result
            if data_batch.size > 0:
                data.append(data_batch)
                labels.append(labels_batch)
            sample_count += 1

            # for reasons of low memory only keep chunk in memory
            if sample_count >= chunk_size:
                data = np.concatenate(data, axis=0)
                labels = np.concatenate(labels, axis=0)

                data_file = os.path.join(chunk_dir, f'data_chunk_{chunk_index}.npy')
                label_file = os.path.join(chunk_dir, f'labels_chunk_{chunk_index}.npy')

                np.save(data_file, data)
                np.save(label_file, labels)

                # free memory
                data = []
                labels = []
                sample_count = 0
                chunk_index += 1

    # save remainder and free mem
    if sample_count > 0:
        data = np.concatenate(data, axis=0)
        labels = np.concatenate(labels, axis=0)

        data_file = os.path.join(chunk_dir, f'data_chunk_{chunk_index}.npy')
        label_file = os.path.join(chunk_dir, f'labels_chunk_{chunk_index}.npy')

        np.save(data_file, data)
        np.save(label_file, labels)

        data = []
        labels = []

    pool.close()
    pool.join()

    return merge_chunks(path)


def merge_chunks(path):
    chunk_dir = os.path.join(path, 'chunks')
    chunk_files = sorted([f for f in os.listdir(chunk_dir) if f.startswith('data_chunk')])

    data = []
    labels = []

    for chunk_file in chunk_files:
        data_chunk = np.load(os.path.join(chunk_dir, chunk_file))
        label_chunk = np.load(os.path.join(chunk_dir, chunk_file.replace('data', 'labels')))
        data.append(data_chunk)
        labels.append(label_chunk)

    # combine several arrays
    data = np.concatenate(data, axis=0)
    labels = np.concatenate(labels, axis=0)

    # save file
    np.save(os.path.join(path, 'data.npy'), data)
    np.save(os.path.join(path, 'labels.npy'), labels)

    print(f"saved data.npy with shape {data.shape}")
    print(f"saved labels.npy with shape {labels.shape}")

    return data, labels


def process_sample(line: str, view_size: int):
    data = []
    labels = []
    # li = line.split(',')            # cannot use split(',') since some file names contain this symbol
    p = line[:-3]                 # remove 0 or 1 and ,
    classification = line[-2]

    for view in stl_to_multiview(p.removesuffix('"').removeprefix('"'), view_size):
        if view is not None:
            data.append(view)
            labels.append(int(classification))

    return np.array(data), np.array(labels)


build_dataset("../../data/convert/gt/screw_or_not/")

