# idea of this file is:
# a) step files of screws are placed in a true folder
# b) step files of non_screw_parts are placed in a false folder
# dataset is generated by transformation via point_cloud to numpy array
import os.path
import numpy as np
import multiprocessing
from functools import partial

from src.generate_datasets.merge_chunks import merge_chunks
from src.point_cloud.stl_to_multiview import stl_to_multiview
from src.generate_ground_truth.build_dictionaries import make_screw_gt_dictionary


def build_gt_dataset(path, num_views=3, view_size=80, chunk_size=100):  # saving every chunk_size samples reduces memory usage
    data = []
    labels = []
    dict_path = make_screw_gt_dictionary(path, 'screw', 'no_screw')

    # mkdir
    chunk_dir = os.path.join(path, "chunks")
    os.makedirs(chunk_dir, exist_ok=True)

    pool = multiprocessing.Pool()

    with open(dict_path, mode='r') as file:
        sample_count = 0
        chunk_index = 0
        lines = file.readlines()

        results = pool.map(partial(process_sample, view_size=view_size), lines)

        for i, result in enumerate(results):
            data_batch, labels_batch = result
            if data_batch.size > 0:
                data.append(data_batch)
                labels.append(labels_batch)
            sample_count += 1

            # for reasons of low memory only keep chunk in memory
            if data and labels and sample_count >= chunk_size:
                data = np.concatenate(data, axis=0)
                labels = np.concatenate(labels, axis=0)

                data_file = os.path.join(chunk_dir, f'data_chunk_{chunk_index}.npy')
                label_file = os.path.join(chunk_dir, f'labels_chunk_{chunk_index}.npy')

                np.save(data_file, data)
                np.save(label_file, labels)

                # free memory
                data = []
                labels = []
                sample_count = 0
                chunk_index += 1

    # save remainder and free mem
    if data and labels and sample_count > 0:
        data = np.concatenate(data, axis=0)
        labels = np.concatenate(labels, axis=0)

        data_file = os.path.join(chunk_dir, f'data_chunk_{chunk_index}.npy')
        label_file = os.path.join(chunk_dir, f'labels_chunk_{chunk_index}.npy')

        np.save(data_file, data)
        np.save(label_file, labels)

        data = []
        labels = []

    pool.close()
    pool.join()

    return merge_chunks(path)


def process_sample(line: str, view_size: int):
    data = []
    labels = []        # labels are here ground_truth values
    # li = line.split(',')            # cannot use split(',') since some file names contain this symbol
    p = line[:-3]                 # remove 0 or 1 and ,
    classification = line[-2]

    for view in stl_to_multiview(p.removesuffix('"').removeprefix('"'), view_size):
        if view is not None:
            data.append(view)
            labels.append(int(classification))

    return np.array(data), np.array(labels)




